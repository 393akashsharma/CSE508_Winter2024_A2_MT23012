{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvQ08Z3EK03u"
      },
      "source": [
        "**Assignment-2-IR-part-1**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing library**"
      ],
      "metadata": {
        "id": "xtFYHgw4FcMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import requests\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "XCWGVMVeFWvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**connecting googgle drive**"
      ],
      "metadata": {
        "id": "Ka-SttW8FwOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WMWFpXI3YggX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "dUFAwbkmHvd4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR3RrtkkKzA4"
      },
      "outputs": [],
      "source": [
        "csv_file = \"/content/drive/MyDrive/A2_Data.csv\"  # Change this to the path of your CSV file\n",
        "data = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqZsqET3Kxmb"
      },
      "outputs": [],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing of image using vgg16"
      ],
      "metadata": {
        "id": "WAf587BsGFJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image_from_url(image_url):\n",
        "    try:\n",
        "        # Remove any extra characters like single quotes, double quotes, and spaces\n",
        "        image_url = image_url.strip(\"[]\\\"' \")\n",
        "\n",
        "        # Check if the URL starts with 'http://' or 'https://'\n",
        "        if not image_url.startswith(\"http://\") and not image_url.startswith(\"https://\"):\n",
        "            image_url = \"http://\" + image_url  # Add 'http://' if missing\n",
        "\n",
        "        print(\"Downloading image from:\", image_url)  # Print the URL for debugging\n",
        "\n",
        "        # Send a GET request to download the image\n",
        "        response = requests.get(image_url)\n",
        "        if response.status_code == 200:\n",
        "            # Decode the content of the image\n",
        "            image_data = response.content\n",
        "            # Convert image data to numpy array\n",
        "            nparr = np.frombuffer(image_data, np.uint8)\n",
        "            # Read image from numpy array\n",
        "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "            # Resize image to the input size of VGG16 (224x224)\n",
        "            resized_image = cv2.resize(image, (224, 224))\n",
        "\n",
        "            # # Convert BGR to RGB (VGG16 expects RGB images)\n",
        "            resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # # Normalize pixel values to range [0, 1]\n",
        "            resized_image = resized_image.astype(\"float32\") / 255.0\n",
        "\n",
        "            # Apply preprocess_input from VGG16\n",
        "            preprocessed_image = preprocess_input(resized_image)\n",
        "\n",
        "            return preprocessed_image\n",
        "        else:\n",
        "            print(\"Failed to download image from:\", image_url)\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return None\n",
        "\n",
        "\n",
        "# Path to the CSV file containing the dataset\n",
        "csv_file = \"/content/drive/MyDrive/A2_Data.csv\"\n",
        "\n",
        "# Read CSV file\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Define an empty dictionary to store the data\n",
        "data_dict = {}\n",
        "\n",
        "# Load pre-trained VGG16 model without the top classification layer\n",
        "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Create a model with VGG16 up to a certain layer\n",
        "feature_extractor = Model(inputs=vgg16.input, outputs=vgg16.get_layer('block5_conv2').output)\n",
        "\n",
        "# Process each row in the dataset\n",
        "for index, row in data.iterrows():\n",
        "    # Extract image URLs from the 'Image' column\n",
        "    image_urls_str = row['Image']\n",
        "    # Split the string by comma to extract individual URLs\n",
        "    image_urls = [url.strip(\"[]\\\"'\") for url in image_urls_str.strip(\"[]\").replace(\"'\", \"\").split(\", \")]\n",
        "    # Process each image URL\n",
        "    for image_url in image_urls:\n",
        "        # Preprocess the image\n",
        "        preprocessed_image = preprocess_image_from_url(image_url)\n",
        "        if preprocessed_image is not None:\n",
        "            # Expand dimensions to match the input shape of VGG16\n",
        "            preprocessed_image = np.expand_dims(preprocessed_image, axis=0)\n",
        "            # Extract features using VGG16 model\n",
        "            features = feature_extractor.predict(preprocessed_image)\n",
        "            # Flatten the features\n",
        "            features = features.flatten()\n",
        "            # Store the image URL and its corresponding feature vector in the dictionary\n",
        "            data_dict[image_url] = features"
      ],
      "metadata": {
        "id": "7CQGqOy-89Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_dict)"
      ],
      "metadata": {
        "id": "GfjlVjFyG6fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sie8_FIyypOb",
        "outputId": "b6967ef4-2819-47c8-8255-cb81cae5de45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_file_path = \"/content/image_feature.pickle\"\n",
        "\n",
        "# Open the file in binary write mode\n",
        "with open(pickle_file_path, \"wb\") as pickle_file:\n",
        "    # Dump the dictionary into the pickle file\n",
        "    pickle.dump(data_dict, pickle_file)\n",
        "print(\"Data has been saved to\", pickle_file_path)"
      ],
      "metadata": {
        "id": "InUBmfA-pOgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}